 I'll read an extra of an assignment. A watch test was conducted to prepare some of efficiency with the null hypothesis H0 stating that the min steps to convergence are equal for both models. The results are Mains 3x3, H0 cannot be rejected with 90% of confidence. Figure 1 shows nearly identical min steps for both models, Mains 5x5, H0 is rejected with 97% of confidence. Dynacue is more efficient, so figure 1 shows some overlap in error bounds, indicating cashier comparable performance by q-learning. Mains 8x8, H0 is rejected with 87% of confidence. Dynacue is more efficient, with narrow error bounds in video 1. Dynacue demonstrates super-resumble efficiency, particularly in larger mazes. For smaller mazes, it's advantage diminishes due to minimal real interactions to the client.